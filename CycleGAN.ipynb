{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    #https://arxiv.org/abs/1607.08022\n",
    "    def __init__(self, epsilon=0.001):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        inverseStdDev = tf.math.rsqrt(variance + self.epsilon)\n",
    "        normalized = (x - mean) * inverseStdDev\n",
    "        return self.gamma * normalized + self.beta\n",
    "    \n",
    "\n",
    "def get_resnet_generator(num_input_channels=3, num_output_channels=3):\n",
    "    ngf = 64\n",
    "    kernel_size = 4\n",
    "    n_blocks = 9\n",
    "    n_downsampling_steps = 2\n",
    "\n",
    "    def get_resiudal_block(inputs, n_filters, dropout=False):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=kernel_size, strides=1, padding='SAME', activation=None)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        if dropout:\n",
    "            x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=kernel_size, strides=1, padding='SAME', activation=None)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = x + inputs\n",
    "        return x\n",
    "\n",
    "    class ResidualBlock(tf.keras.layers.Layer):\n",
    "        def __init__(self, n_filters):\n",
    "            super(ResidualBlock, self).__init__()\n",
    "            self.n_filters = n_filters\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            inputs = tf.keras.Input(input_shape[1:])\n",
    "            x = get_resiudal_block(inputs, self.n_filters)\n",
    "            self.block = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "        def call(self, inputs):\n",
    "            return self.block(inputs)\n",
    "\n",
    "\n",
    "    layers = [\n",
    "        tf.keras.Input((None,None,num_input_channels)),\n",
    "        tf.keras.layers.Conv2D(filters=ngf, kernel_size=8, padding='SAME', activation=None),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU()\n",
    "        ]\n",
    "\n",
    "\n",
    "    for i in range(n_downsampling_steps):\n",
    "        layers += [ \n",
    "            tf.keras.layers.Conv2D(filters=ngf*2*(2**i), kernel_size=kernel_size, strides=2, padding='SAME', activation=None),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ]\n",
    "\n",
    "    for i in range(n_blocks):    \n",
    "        layers += [ResidualBlock(ngf*(2**n_downsampling_steps))]\n",
    "\n",
    "    for i in range(n_downsampling_steps):\n",
    "        n_filters = ngf * (2**(n_downsampling_steps - i - 1))\n",
    "        layers += [ \n",
    "            tf.keras.layers.Conv2DTranspose(filters=n_filters, kernel_size=4, strides=2, padding='SAME', activation=None),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ]\n",
    "\n",
    "    layers += [\n",
    "        tf.keras.layers.Conv2DTranspose(filters=num_output_channels, kernel_size=8, padding='SAME', activation=None),\n",
    "        tf.keras.layers.Lambda(lambda el: tf.keras.activations.tanh(el))\n",
    "        ]\n",
    "\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def get_discriminator(num_output_channels=3):\n",
    "    ndf = 64\n",
    "    kernel_size = 4\n",
    "    n_layers = 4\n",
    "    n_layers -= 1\n",
    "\n",
    "    layers = [\n",
    "        tf.keras.Input((None,None,num_output_channels)),\n",
    "        tf.keras.layers.Conv2D(filters=ndf, kernel_size=kernel_size, strides=2, padding='SAME', activation=None),\n",
    "        tf.keras.layers.LeakyReLU(0.2)\n",
    "    ]\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        layers += [\n",
    "            tf.keras.layers.Conv2D(filters=ndf * (2**i), kernel_size=kernel_size, strides=2, padding='SAME', activation=None),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(0.2)\n",
    "        ]\n",
    "        \n",
    "    layers += [\n",
    "        tf.keras.layers.Conv2D(filters=ndf * (2**n_layers), kernel_size=kernel_size, strides=1, padding='SAME', activation=None),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=1, padding='SAME', activation=None)\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "\n",
    "def normalize(img):\n",
    "    return (tf.cast(img, tf.float32) / 128.0) - 1.0\n",
    "\n",
    "def norm_gen_output(img):\n",
    "    return (img + 1) / 2\n",
    "\n",
    "def resize_and_crop(img):\n",
    "    img = tf.image.resize(img, (IMAGE_SIZE+30,IMAGE_SIZE+30))\n",
    "    img = tf.image.random_crop(img, (IMAGE_SIZE,IMAGE_SIZE,img.shape[-1]))\n",
    "    return img\n",
    "\n",
    "def perp_train_ds(ds):\n",
    "    ds = ds.map(lambda a,b: a)\n",
    "    ds = ds.map(normalize)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.map(resize_and_crop)\n",
    "    ds = ds.map(tf.image.random_flip_left_right)\n",
    "    ds = ds.shuffle(1000)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(1)\n",
    "    return ds\n",
    "\n",
    "def perp_test_ds(ds):\n",
    "    ds = ds.map(lambda a,b: a)\n",
    "    ds = ds.map(normalize)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(1000)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(1)\n",
    "    return ds\n",
    "\n",
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra',  with_info=True, as_supervised=True)\n",
    "#dataset, metadata = tfds.load('cycle_gan/vangogh2photo',  with_info=True, as_supervised=True)\n",
    "#dataset, metadata = tfds.load('cycle_gan/monet2photo',  with_info=True, as_supervised=True)\n",
    "\n",
    " \n",
    "train_A, train_B = dataset['trainB'], dataset['trainA']\n",
    "test_A, test_B = dataset['testB'], dataset['testA']\n",
    "\n",
    "train_A = perp_train_ds(train_A)\n",
    "train_B = perp_train_ds(train_B)\n",
    "\n",
    "test_A = perp_test_ds(test_A)\n",
    "test_B = perp_test_ds(test_B)\n",
    "\n",
    "iter_A = (iter(train_A))\n",
    "iter_B = (iter(train_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = next(iter_A)[0]\n",
    "img_B = next(iter_B)[0]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(norm_gen_output(img_A))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(norm_gen_output(img_B))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_crossentropy_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(disc_out_real, disc_out_generated):\n",
    "    real_loss = binary_crossentropy_loss(tf.ones_like(disc_out_real), disc_out_real)\n",
    "    generated_loss = binary_crossentropy_loss(tf.zeros_like(disc_out_generated), disc_out_generated)\n",
    "    return (real_loss + generated_loss) / 2.0\n",
    "\n",
    "def generator_loss(disc_out_generated):\n",
    "    return binary_crossentropy_loss(tf.ones_like(disc_out_generated)+0.0, disc_out_generated)\n",
    "\n",
    "def mean_absolute_error(a,b):\n",
    "    return tf.reduce_mean(tf.abs(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_A = get_resnet_generator()\n",
    "generator_B = get_resnet_generator()\n",
    "\n",
    "discriminator_A = get_discriminator()\n",
    "discriminator_B = get_discriminator()\n",
    "\n",
    "generator_optimizer_A = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_optimizer_B = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_optimizer_A = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer_B = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_A = next(iter_A)\n",
    "batch_B = next(iter_B)\n",
    "generator_A(batch_A)\n",
    "generator_B(batch_B)\n",
    "\n",
    "discriminator_A(batch_B)\n",
    "discriminator_A(batch_A)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(batch_A, batch_B):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        generated_B = generator_A(batch_A, training=True)\n",
    "        cycle_A = generator_B(generated_B, training=True)\n",
    "    \n",
    "        generated_A = generator_B(batch_B, training=True)\n",
    "        cycle_B = generator_A(generated_A, training=True)\n",
    "    \n",
    "\n",
    "        cycle_loss = (mean_absolute_error(batch_A, cycle_A) + mean_absolute_error(batch_B, cycle_B)) / 2.0\n",
    "    \n",
    "        disc_out_real_A = discriminator_A(batch_B, training=True)\n",
    "        disc_out_generated_A = discriminator_A(generated_B, training=True)\n",
    "    \n",
    "        disc_out_real_B = discriminator_B(batch_A, training=True)\n",
    "        disc_out_generated_B = discriminator_B(generated_A, training=True)\n",
    "\n",
    "        discriminator_loss_A = discriminator_loss(disc_out_real_A, disc_out_generated_A)\n",
    "        discriminator_loss_B = discriminator_loss(disc_out_real_B, disc_out_generated_B)\n",
    "\n",
    "        identity_loss_A = mean_absolute_error(batch_B, generator_A(batch_B, training=True))\n",
    "        identity_loss_B = mean_absolute_error(batch_A, generator_B(batch_A, training=True))\n",
    "        \n",
    "        generator_loss_A = 10 * cycle_loss + generator_loss(disc_out_generated_A) + 5 * identity_loss_A\n",
    "        generator_loss_B = 10 * cycle_loss + generator_loss(disc_out_generated_B) + 5 * identity_loss_B\n",
    "        \n",
    "        \n",
    "        \n",
    "    gen_grads_A = tape.gradient(generator_loss_A, generator_A.trainable_variables)\n",
    "    gen_grads_B = tape.gradient(generator_loss_B, generator_B.trainable_variables)\n",
    "\n",
    "    generator_optimizer_A.apply_gradients(zip(gen_grads_A, generator_A.trainable_variables))\n",
    "    generator_optimizer_B.apply_gradients(zip(gen_grads_B, generator_B.trainable_variables))\n",
    "\n",
    "    disc_grads_A = tape.gradient(discriminator_loss_A, discriminator_A.trainable_variables)\n",
    "    disc_grads_B = tape.gradient(discriminator_loss_B, discriminator_B.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer_A.apply_gradients(zip(disc_grads_A, discriminator_A.trainable_variables))\n",
    "    discriminator_optimizer_B.apply_gradients(zip(disc_grads_B, discriminator_B.trainable_variables))\n",
    "    return cycle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 20)\n",
    "for epoche in range(100):\n",
    "    for step in range(20):\n",
    "        batch_A = next(iter_A)\n",
    "        batch_B = next(iter_B)\n",
    "        cycle_loss = train(batch_A, batch_B)\n",
    "        losses.append(cycle_loss)\n",
    "        print(step)\n",
    "    batch_A = next(iter_A)\n",
    "    batch_B = next(iter_B)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(norm_gen_output(batch_A)[0])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(norm_gen_output(generator_A(batch_A, training=True))[0])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(norm_gen_output(batch_A)[0])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(norm_gen_output(generator_B(generator_A(batch_A, training=True), training=True))[0])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GeneratorModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, None,None,3], tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        #x = tf.expand_dims(x,0)\n",
    "        return self.model(x, training=True) #[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(GeneratorModel(generator_A), './generator_A_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(GeneratorModel(generator_B), './generator_B_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('./generator_A_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(norm_gen_output(model(cv2.resize(img[0].numpy(),(0,0), fx=6, fy=6)[np.newaxis,...])[0]))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(norm_gen_output(img[0]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
